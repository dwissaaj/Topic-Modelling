{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:48: DeprecationWarning: invalid escape sequence \\d\n",
      "<>:48: DeprecationWarning: invalid escape sequence \\d\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp/ipykernel_7532/3251626927.py:48: DeprecationWarning: invalid escape sequence \\d\n",
      "  '\\d+':'',\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'xx_ent_wiki_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_7532/3251626927.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mpyLDAvis\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgensim_models\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mspacy\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mspacy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"xx_ent_wiki_sm\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdisable\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'parser'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'ner'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[0mw_tokenizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnltk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtokenize\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mWhitespaceTokenizer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\pc\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages\\spacy\\__init__.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(name, vocab, disable, exclude, config)\u001B[0m\n\u001B[0;32m     49\u001B[0m     \u001B[0mRETURNS\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mLanguage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mThe\u001B[0m \u001B[0mloaded\u001B[0m \u001B[0mnlp\u001B[0m \u001B[0mobject\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     50\u001B[0m     \"\"\"\n\u001B[1;32m---> 51\u001B[1;33m     return util.load_model(\n\u001B[0m\u001B[0;32m     52\u001B[0m         \u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvocab\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mvocab\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdisable\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdisable\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mexclude\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mexclude\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     53\u001B[0m     )\n",
      "\u001B[1;32mc:\\users\\pc\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages\\spacy\\util.py\u001B[0m in \u001B[0;36mload_model\u001B[1;34m(name, vocab, disable, exclude, config)\u001B[0m\n\u001B[0;32m    352\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mOLD_MODEL_SHORTCUTS\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    353\u001B[0m         \u001B[1;32mraise\u001B[0m \u001B[0mIOError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mErrors\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mE941\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfull\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mOLD_MODEL_SHORTCUTS\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 354\u001B[1;33m     \u001B[1;32mraise\u001B[0m \u001B[0mIOError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mErrors\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mE050\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    355\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    356\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mOSError\u001B[0m: [E050] Can't find model 'xx_ent_wiki_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "mport nltk\n",
    "\n",
    "import pyLDAvis\n",
    "\n",
    "import pyLDAvis.gensim_models\n",
    "import spacy\n",
    "#model = spacy.load(\"xx_ent_wiki_sm\", disable=['parser', 'ner'])\n",
    "import en_core_web_sm\n",
    "\n",
    "model = en_core_web_sm.load()\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "stopwords = stopwords.words(\"indonesian\")\n",
    "\n",
    "\n",
    "\n",
    "#data = load_data(\"file.json\")\n",
    "\n",
    "encoding = 'unicode_escape'\n",
    "data = pd.read_csv(\"tweet.csv\", encoding= 'unicode_escape', sep='delimiter', header=None)\n",
    "data = data.rename(columns={0: 'teks'})\n",
    "\n",
    "data['cols_to_check'] = data['teks'].replace({\n",
    "                                              '\"':'',\n",
    "                                              '\\d+':'',\n",
    "                                              ':':'',\n",
    "                                              ';':'',\n",
    "                                              '#':'',\n",
    "                                              '@':'',\n",
    "                                              '_':'',\n",
    "                                                ',': '',\n",
    "                                                \"'\": '',\n",
    "                                              }, regex=True)\n",
    "data['check'] = data['cols_to_check'].str.replace(r'[https]+[?://]+[^\\s<>\"]+|www\\.[^\\s<>\"]+[@?()]+[(??)]+[)*]+[(\\xa0]+[-&gt...]', \"\",regex=True)\n",
    "\n",
    "data['clean'] = data['check'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "data['lemma'] = data.clean.apply(lemmatize_text)\n",
    "\n",
    "lemma = data['lemma']\n",
    "\n",
    "\n",
    "id2word = corpora.Dictionary(lemma)\n",
    "\n",
    "corpus = []\n",
    "for text in lemma:\n",
    "    new = id2word.doc2bow(text)\n",
    "    corpus.append(new)\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=20,\n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto')\n",
    "\n",
    "py = pyLDAvis.gensim_models.prepare(lda_model,corpus,id2word,mds='mmds',R=30)\n",
    "\n",
    "extract = pyLDAvis.save_html(py,\"done.html\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}